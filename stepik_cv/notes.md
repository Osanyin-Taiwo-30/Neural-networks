# 1. Нейрон и нейронная сеть
Простейшая функция активации: `f(z) = 1 if z > 0 else 0`

Как выглядит разделяющая поверхность (область, в которой происходит смена значения функции активации)?

 `z = w_0 * x_0 + ... + w_(N - 1) * x_(N - 1) + b`
 
 разделяющая поверхность: `w_0 * x_0 + ... + w_(N - 1) * x_(N - 1) + b = 0`

 Данное уравнение задает гиперплоскость, нормальную вектору весов `w`. Функция активации равна 1 с той стороны гиперплоскости, в которую направлен вектор весов `w`
 
 Примеры задач, которые можно решить с помощью одного нейрона: NOT, AND, OR. Смоделировать XOR не получится, поскольку в этом случае данные не являются линейно разделимыми.
 Однако `x ^ y = (~x & y) | (x & ~y)`, следовательно, XOR можно смоделировать с помощью нескольких нейронов. Также XOR можно смоделировать и тремя нейронами: `x1 XOR x2 = f(f(x1 - x2) + f(x2 - x1))`
 
 Таким образом, объединением нейронов в сеть можно строить нелинейные разделяющие поверхности. Более того, эти поверхности могут формировать несвязанные области
 
 Имеет ли смысл соединять несколько нейронов друг за другом, имеющих линейную активационную функцию?
 
 Нет, в таком сценарии преобразование сети можно развернуть и показать, что оно эквивалентно линейной регрессии с небольшим числом параметров
 
# 2. Построение нейронной сети

[При помощи линейной комбинации сигмоид можно с любой точностью приблизить любую ограниченную функцию с не более, чем счетным числом разрывов](https://ru.wikipedia.org/wiki/%D0%A2%D0%B5%D0%BE%D1%80%D0%B5%D0%BC%D0%B0_%D0%A6%D1%8B%D0%B1%D0%B5%D0%BD%D0%BA%D0%BE)

Рассмотрим линейную регрессию: `y_hat = X * a`

Будем решать задачу при помощи минимизации L2-нормы: `||X * a - y|| ** 2 -> min`

Решение задачи в матричном виде: `a = (X.T * X) ** (-1) * X.T * y`

Если количество признаков в матрице `X` больше, чем количество наблюдаемых примеров, решение задачи оптимизации не единственно: существует целое векторное пространство, добавление любого элемента из которого к любому решению задачи минимизации приводит к другому решению той же задачи.

Пусть `a = (X.T * X) ** (-1) * X.T * y + a_0`

`y = X * a`

`y = X * (X.T * X) ** (-1) * X.T * y + X * a_0`

`X.T * y = (X.T * X) * (X.T * X) ** (-1) * X.T * y + X.T * X * a_0`

`X.T * X * a_0 = 0`

Таким образом, добавлением любого собственного вектора матрицы `X.T * X`, соответствующего нулевому собственному значению, к оптимальному вектору весов `a` приводит к другому оптимальному решению.

Рассмотрим нейронную сеть, которая состоит из некоторого количества полносвязанных слоев с произвольной функцией активации.

Рассмотрим некоторый слой этой нейронной сети, в котором имеется `M + 1` нейронов. Он принимает на вход некоторый вектор `x`  размерности `N + 1`, который вычисляется предшествующими слоями, а возвращает вектор `y` размерности `M`, который затем обрабатывается последующими слоями:

`y = f(W.T * x)`

`W` - матрица векторов весов для всех нейронов данного слоя + вектор смещений

если имеется несколько входных векторов в данный нейронный слой, выражение можно переписать в виде:

`Y = f(W.T * X)`

Найдем векторное пространство весов модели, добавление любого элемента которого к вектору параметров любого нейрона данного слоя, не приводит к изменению работы данного слоя (и, следовательно, всей сети) ни на одном из векторов обучающей выборки

`f((W + W_0).T * X) = f(W.T * X)`

поскольку функция активации может быть любой:

`(W + W_0).T * X = W.T * X` или `W_0.T * X = 0`

рассмотрим один из векторов `w_0` матрицы `W_0`:

`w_0.T * X = 0`

матрица `X` не обязательно квадратная, домножим обе части на `X.T`:

`w_0.T * (X * X.T) = 0`

Таким образом, `w_0` - левый собственный вектор матрицы `X * X.T` с нулевым собственным значением. Он является так же и (правым) собственным вектором, соответствующим нулевому собственному значению для матрицы `(X * X.T).T = X * X.T`

Добавление `w_0` к вектору параметров любого нейрона данного слоя не приведет к изменению активаций сети

# 3. Задачи, решаемые при помощи нейронных сетей

## 3.1 Локализация и детекция
 
Необходимо предсказывать вероятность `p` нахождения объекта на изображении + параметры bounding box'a (координаты центра `(x_c, y_c)`, длина `h` и ширина `w`) => нейросеть с 5 выходами

* Ограничения:
    * На изображении не больше одного объекта
    * Объект есть на изображении, если его центр лежит в пределах изображения
    * Объект может выходить за пределы изображения
* Функции активации:
    * `p = sigma(y_0)`
    * `x_c = sigma(y_1)` (задаем изображение в координатах 0 и 1)
    * `y_c = sigma(y_2)`
    * `w = exp(y_3)` (хороша тем, что не ограничена сверху)
    * `h = exp(y_4)`
* Функции потерь: 
    * `1` - индикатор наличия объекта на изображении
    * `p -> BCE(sigma(y_0), 1)`
    * `x_c -> BCE(sigma(y_1), x_c)`
    * `y_c -> BCE(sigma(y_2), y_c)`
    * `w -> MSE(y_3, log w)`
    * `h -> MSE(y_4, log h)`

Логарифм для `w` и `h` позволяет одинаково штрафовать одинаковые масштабы ошибки. 

Пусть ошибаемся на 10%: `y_hat = 1.1 * y => log y_hat = log 1.1 + log y`

Ошибка: `log y_hat - log y = log 1.1` вне зависимости от `y`

## 3.2 Сегментация

Необходимо отделить те пиксели, на которых есть объект, от тех пикселей, где объекта нет

Сеть должна выдавать карту скоров, есть ли в данном пикселе или нет фрагмент объекта => выход сети `sigma(y)`

Функций потерь - попиксельная кросс-энтропия: `sum_j BCE(sigma(y_j), t_j)`

## 3.3 Сжатие размерности

Encoder-Decoder архитектура

Функции потерь: `MSE`, попиксельная `BCE` (если интенсивность пикселей от 0 до 1), могут быть и более сложные

## 3.4 SuperResolution

Обучение: сжимаем исходную картинку -> прогон через сеть -> сравнение выхода сети с исходной

# 4. Методы оптимизации

## 4.0 Градиентный спуск
* вычисляем loss на всей выборке
* проблема: необходимо считать значение и градиент loss-функции по всей выборке => много времени на один шаг

## 4.1 Стохастический градиентный спуск
* вычисляем loss по одному примеру 
* проблема: пример может быть неправильно размечен, положительные и отрицательные примеры могут чередоваться, их градиенты будут направлены в разные стороны => неустойчивость

## 4.2 Minibatch градиентный спуск
* вычисляем loss по M (1 << M << размер выборки) объектам
* проблема: вытянутые функции потерь приводят к осцилляциям относительно большой полуоси

## 4.3 Momentum
* `w_(t + 1) = w_t + a * v_t` 
* `v_(t + 1) = b * v_t - grad f(w_t)`
* при градиентных шагах по вытянутой функции потерь проекция скорости на малую полуось будет осциллировать относительно нуля, проекция же на большу полуось будет накапливаться => шаг в направлении большой полуоси будет увеличиваться, и метод сойдется быстрее

## 4.4 Nesterov accelerated gradient
* модификация Momentum: `grad f` вычисляется не в предыдущей точке (`w_t`), а в той, в которую бы мы попали, если бы сделали шаг с той же скоростью (`v_t`)
* таким образом, метод позволяет "не врезаться в стену"
* `w_(t + 1) = w_t + a * v_t` 
* `v_(t + 1) = b * v_t - grad f(w_t + a * v_t)`

## 4.5 Exponential moving average (EMA)
* цель та же, что и у Momentum: найти направление, вдоль которого градиент не осциллирует
* `w_(t + 1) = w_t - a * EMA[(grad f)_t]`
* `EMA[f_t] = (1 - b) * f_t + b * EMA[f_(t - 1)]`

## 4.6 Rprop
* меняем параметры независимо
* `w_(t + 1)_i = w_t_i - a_t_i * sign[(grad f_t)_i ]`
* если произведение i-ой компоненты градиента функции потерь в соседние моменты времени больше нуля, увеличиваем `a_i`, иначе - уменьшаем
* проблема: плохо работает с батчами

## 4.7 RMSprop
* `w_(t + 1) = w_t - a * grad f(w_t) / sqrt[EMA{(grad f) ** 2}]` 
* если по какому-либо параметру функция меняется слишком сильно - стоит спускаться медленнее, если же функция по данному параметру меняется слабо - стоит увеличить шаг

## 4.8 Adam
* метод RMSprop имеет адаптивный шаг для каждого параметра, но напоминает SGD, его можно улучшить с помощью EMA
* `w_(t + 1) = w_t - a * EMA{grad f_t, b1} / sqrt[EMA{(grad f_t) ** 2, b2}]`
* как правило, оптимальные параметры: `a = 3e-4, b1=0.9, b2=0.999`

# 5. Сверточные нейронные сети

Padding: дополнение нулями границ изображения

Stride: шаг, с которым применяется ядро свертки

В каждом слое CNN применяется n разных ядер свертки -> на следующий слой поступает n-канальное изображение

Pooling: операция агрегации над каналом изображения после свертки (применяется к каждому каналу независимо)

Основной benchmark: ImageNet 1000 (15 миллионов изображений, 1000 классов)

Архитектуры:

* [LeNet](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf) (1998)
    * предложил Yann LeCun для распознавания рукописных цифр (MNIST)
    * 2 сверточных слоя -> flatten -> 3 полносвязных + softmax
* [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) (2012)
    * designed by Alex Krizhevsky (Geoffrey Hinton PhD Student)
    * из картинки случайным образом вырезалась квадратная область и масштабировалась к размеру 224x224 с помощью билинейной интерполяции
    * 5 сверточных слоев -> 3 полносвязных -> softmax
    * последние 2 сверточных слоя без max_pooling и со свертками 3x3 (у авторов не хватало памяти, чтобы прогнать через свертку 5x5 даже одну картинку). Receptive field такой же, а параметров меньше
    * `d/dx [sigmoid] <= 0.25` => градиент на первых слоях будет мал, слои будут медленно учиться (затухание градиента). Была предложена нелинейность ReLU (сейчас также существуют ELU, leaky ReLU и SeLU)
* [VGG](https://arxiv.org/pdf/1409.1556.pdf) (2014)
    * Visual Geometry Group, University of Oxford
    * Обучаем сеть из 11 слоев -> добавляем 2 глубоких слоя -> обучаем из 13 слоев -> ...
    * Такой подход позволяет компенсировать затухающие градиенты
    * max_pooling применяется после каскада сверток (чтобы увеличивать receptive field)
    * VGG часто используют в качестве back bone для других задач (замораживают сверточные слои, убирают полносвязные, добавляют свои слои и дообучают)
* [GoogLeNet](https://arxiv.org/pdf/1409.4842.pdf) (2015)
    * Inception block: к выходу предыдущего слоя параллельно применяется несколько каскадов сверток с разными ядрами. Затем результаты всех фильтров конкатенируются в многоканальное изображение
    * Свертки 1x1: позволяют уменьшить количество каналов
    * имеет 3 Loss-функции на разных участках сети: если градиент по параметрам глубоких слоев от выходной функции потерь крайне мал, он может быть значимым для более ранних loss-функции
* [ResNet](https://arxiv.org/pdf/1512.03385.pdf) (2015)
    * Microsoft Research
    * До сих пор борьба с затуханием градиента происходила только подбором функции активации
    * Авторы предложили Residual Block: выход слоя представляется суммой результата сверточных преобразований и исходного тензора
    * существуют варианты ResNet с количеством слоев от 18 до 1024
    * в ResNet присутствует только один полносвязный слой + softmax в конце
    * нет ограничений на размер изображения, avg_pooling после сверточных слоев усреднит по получившемуся размеру (желательно, чтобы размер был кратен 32, поскольку в сети 5 раз изображение сжимается в 2 раза)

Сейчас архитектура ResNet является де-факто золотым стандартом для задач классификации. Для других задач есть свои стандарты. Остальные архитектуры показаны в качестве примера эволюции научной мысли. Впрочем, трюки из Inception до сих пор используются в некоторых уникальных случаях (например, в сети [ResNeXt](https://arxiv.org/pdf/1611.05431.pdf), которая лучше ResNet'а на классификации, но не набрала должной популярности), а VGG часто используется в качестве получения признаков из изображения, например, в задаче Style Transfer

# 6. Регуляризация и нормализация

Борьба с переобучением:
* брать несложные модели (уменьшать число параметров)
* найти больше данных (аугментация)
* early stopping по валидационной выборке
* регуляризация
    * L2 (Ridge, Тихонова, Weight Decay)
        * В случае большого количества минимумов функции потерь L2-регуляризация сглаживает локальные минимумы. Пример: `L(a) = cos(a) + lambda * a^2`
    * L1 (Lasso)
        * Позволяет отбирать признаки
    * Характерные для нейронных сетей
        * [DropOut](http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf): некоторые из нейронов выдают нулевой результат
            * train-time: каждый из нейронов остается в архитектуре с вероятностью `p`
            * test-time: в сети присутствуют все нейроны, но исходящие веса домножаются на вероятность `p`
        * DropConnect: зануляются случайные веса связей между нейронами
        * Сильно повышает стабильность сети
        * Напоминает аугментацию шумом соль и перец (точнее только соль)
        
[BatchNorm](https://arxiv.org/pdf/1502.03167.pdf):
* train-time:
    * для каждого нейрона приводим его выход к нулевому среднему и единичной дисперсии по батчу: `x = (x - x_avg_batch) / x_std_batch`
    * почему по батчу, а не по всей истории? - Чтобы сделать backprop
    * также для всей сети могут существовать обучаемые параметры `a`, `b`: `x = (x - x_avg_batch) / x_std_batch * a + b`
* test-time
    * `x_avg_batch` и `x_std_batch` заменяются на экспоненциальное среднее от параметров, собранных при обучении: `x_avg = EMA(x_avg_batch)`, `x_std = EMA(x_std_batch)`
