# 1. Нейрон и нейронная сеть
Простейшая функция активации: `f(z) = 1 if z > 0 else 0`

Как выглядит разделяющая поверхность (область, в которой происходит смена значения функции активации)?

 `z = w_0 * x_0 + ... + w_(N - 1) * x_(N - 1) + b`
 
 разделяющая поверхность: `w_0 * x_0 + ... + w_(N - 1) * x_(N - 1) + b = 0`

 Данное уравнение задает гиперплоскость, нормальную вектору весов `w`. Функция активации равна 1 с той стороны гиперплоскости, в которую направлен вектор весов `w`
 
 Примеры задач, которые можно решить с помощью одного нейрона: NOT, AND, OR. Смоделировать XOR не получится, поскольку в этом случае данные не являются линейно разделимыми.
 Однако `x ^ y = (~x & y) | (x & ~y)`, следовательно, XOR можно смоделировать с помощью нескольких нейронов. Также XOR можно смоделировать и тремя нейронами: `x1 XOR x2 = f(f(x1 - x2) + f(x2 - x1))`
 
 Таким образом, объединением нейронов в сеть можно строить нелинейные разделяющие поверхности. Более того, эти поверхности могут формировать несвязанные области
 
 Имеет ли смысл соединять несколько нейронов друг за другом, имеющих линейную активационную функцию?
 
 Нет, в таком сценарии преобразование сети можно развернуть и показать, что оно эквивалентно линейной регрессии с небольшим числом параметров
 
# 2. Построение нейронной сети

[При помощи линейной комбинации сигмоид можно с любой точностью приблизить любую ограниченную функцию с не более, чем счетным числом разрывов](https://ru.wikipedia.org/wiki/%D0%A2%D0%B5%D0%BE%D1%80%D0%B5%D0%BC%D0%B0_%D0%A6%D1%8B%D0%B1%D0%B5%D0%BD%D0%BA%D0%BE)

Рассмотрим линейную регрессию: `y_hat = X * a`

Будем решать задачу при помощи минимизации L2-нормы: `||X * a - y|| ** 2 -> min`

Решение задачи в матричном виде: `a = (X.T * X) ** (-1) * X.T * y`

Если количество признаков в матрице `X` больше, чем количество наблюдаемых примеров, решение задачи оптимизации не единственно: существует целое векторное пространство, добавление любого элемента из которого к любому решению задачи минимизации приводит к другому решению той же задачи.

Пусть `a = (X.T * X) ** (-1) * X.T * y + a_0`

`y = X * a`

`y = X * (X.T * X) ** (-1) * X.T * y + X * a_0`

`X.T * y = (X.T * X) * (X.T * X) ** (-1) * X.T * y + X.T * X * a_0`

`X.T * X * a_0 = 0`

Таким образом, добавлением любого собственного вектора матрицы `X.T * X`, соответствующего нулевому собственному значению, к оптимальному вектору весов `a` приводит к другому оптимальному решению.

Рассмотрим нейронную сеть, которая состоит из некоторого количества полносвязанных слоев с произвольной функцией активации.

Рассмотрим некоторый слой этой нейронной сети, в котором имеется `M + 1` нейронов. Он принимает на вход некоторый вектор `x`  размерности `N + 1`, который вычисляется предшествующими слоями, а возвращает вектор `y` размерности `M`, который затем обрабатывается последующими слоями:

`y = f(W.T * x)`

`W` - матрица векторов весов для всех нейронов данного слоя + вектор смещений

если имеется несколько входных векторов в данный нейронный слой, выражение можно переписать в виде:

`Y = f(W.T * X)`

Найдем векторное пространство весов модели, добавление любого элемента которого к вектору параметров любого нейрона данного слоя, не приводит к изменению работы данного слоя (и, следовательно, всей сети) ни на одном из векторов обучающей выборки

`f((W + W_0).T * X) = f(W.T * X)`

поскольку функция активации может быть любой:

`(W + W_0).T * X = W.T * X` или `W_0.T * X = 0`

рассмотрим один из векторов `w_0` матрицы `W_0`:

`w_0.T * X = 0`

матрица `X` не обязательно квадратная, домножим обе части на `X.T`:

`w_0.T * (X * X.T) = 0`

Таким образом, `w_0` - левый собственный вектор матрицы `X * X.T` с нулевым собственным значением. Он является так же и (правым) собственным вектором, соответствующим нулевому собственному значению для матрицы `(X * X.T).T = X * X.T`

Добавление `w_0` к вектору параметров любого нейрона данного слоя не приведет к изменению активаций сети
