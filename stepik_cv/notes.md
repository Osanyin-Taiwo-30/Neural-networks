# 1. Нейрон и нейронная сеть
Простейшая функция активации: `f(z) = 1 if z > 0 else 0`

Как выглядит разделяющая поверхность (область, в которой происходит смена значения функции активации)?

 `z = w_0 * x_0 + ... + w_(N - 1) * x_(N - 1) + b`
 
 разделяющая поверхность: `w_0 * x_0 + ... + w_(N - 1) * x_(N - 1) + b = 0`

 Данное уравнение задает гиперплоскость, нормальную вектору весов `w`. Функция активации равна 1 с той стороны гиперплоскости, в которую направлен вектор весов `w`
 
 Примеры задач, которые можно решить с помощью одного нейрона: NOT, AND, OR. Смоделировать XOR не получится, поскольку в этом случае данные не являются линейно разделимыми.
 Однако `x ^ y = (~x & y) | (x & ~y)`, следовательно, XOR можно смоделировать с помощью нескольких нейронов. Также XOR можно смоделировать и тремя нейронами: `x1 XOR x2 = f(f(x1 - x2) + f(x2 - x1))`
 
 Таким образом, объединением нейронов в сеть можно строить нелинейные разделяющие поверхности. Более того, эти поверхности могут формировать несвязанные области
 
 Имеет ли смысл соединять несколько нейронов друг за другом, имеющих линейную активационную функцию?
 
 Нет, в таком сценарии преобразование сети можно развернуть и показать, что оно эквивалентно линейной регрессии с небольшим числом параметров
 
# 2. Построение нейронной сети

[При помощи линейной комбинации сигмоид можно с любой точностью приблизить любую ограниченную функцию с не более, чем счетным числом разрывов](https://ru.wikipedia.org/wiki/%D0%A2%D0%B5%D0%BE%D1%80%D0%B5%D0%BC%D0%B0_%D0%A6%D1%8B%D0%B1%D0%B5%D0%BD%D0%BA%D0%BE)

Рассмотрим линейную регрессию: `y_hat = X * a`

Будем решать задачу при помощи минимизации L2-нормы: `||X * a - y|| ** 2 -> min`

Решение задачи в матричном виде: `a = (X.T * X) ** (-1) * X.T * y`

Если количество признаков в матрице `X` больше, чем количество наблюдаемых примеров, решение задачи оптимизации не единственно: существует целое векторное пространство, добавление любого элемента из которого к любому решению задачи минимизации приводит к другому решению той же задачи.

Пусть `a = (X.T * X) ** (-1) * X.T * y + a_0`

`y = X * a`

`y = X * (X.T * X) ** (-1) * X.T * y + X * a_0`

`X.T * y = (X.T * X) * (X.T * X) ** (-1) * X.T * y + X.T * X * a_0`

`X.T * X * a_0 = 0`

Таким образом, добавлением любого собственного вектора матрицы `X.T * X`, соответствующего нулевому собственному значению, к оптимальному вектору весов `a` приводит к другому оптимальному решению.

Рассмотрим нейронную сеть, которая состоит из некоторого количества полносвязанных слоев с произвольной функцией активации.

Рассмотрим некоторый слой этой нейронной сети, в котором имеется `M + 1` нейронов. Он принимает на вход некоторый вектор `x`  размерности `N + 1`, который вычисляется предшествующими слоями, а возвращает вектор `y` размерности `M`, который затем обрабатывается последующими слоями:

`y = f(W.T * x)`

`W` - матрица векторов весов для всех нейронов данного слоя + вектор смещений

если имеется несколько входных векторов в данный нейронный слой, выражение можно переписать в виде:

`Y = f(W.T * X)`

Найдем векторное пространство весов модели, добавление любого элемента которого к вектору параметров любого нейрона данного слоя, не приводит к изменению работы данного слоя (и, следовательно, всей сети) ни на одном из векторов обучающей выборки

`f((W + W_0).T * X) = f(W.T * X)`

поскольку функция активации может быть любой:

`(W + W_0).T * X = W.T * X` или `W_0.T * X = 0`

рассмотрим один из векторов `w_0` матрицы `W_0`:

`w_0.T * X = 0`

матрица `X` не обязательно квадратная, домножим обе части на `X.T`:

`w_0.T * (X * X.T) = 0`

Таким образом, `w_0` - левый собственный вектор матрицы `X * X.T` с нулевым собственным значением. Он является так же и (правым) собственным вектором, соответствующим нулевому собственному значению для матрицы `(X * X.T).T = X * X.T`

Добавление `w_0` к вектору параметров любого нейрона данного слоя не приведет к изменению активаций сети

# 3. Задачи, решаемые при помощи нейронных сетей

## 3.1 Локализация и детекция
 
Необходимо предсказывать вероятность `p` нахождения объекта на изображении + параметры bounding box'a (координаты центра `(x_c, y_c)`, длина `h` и ширина `w`) => нейросеть с 5 выходами

* Ограничения:
    * На изображении не больше одного объекта
    * Объект есть на изображении, если его центр лежит в пределах изображения
    * Объект может выходить за пределы изображения
* Функции активации:
    * `p = sigma(y_0)`
    * `x_c = sigma(y_1)` (задаем изображение в координатах 0 и 1)
    * `y_c = sigma(y_2)`
    * `w = exp(y_3)` (хороша тем, что не ограничена сверху)
    * `h = exp(y_4)`
* Функции потерь: 
    * `1` - индикатор наличия объекта на изображении
    * `p -> BCE(sigma(y_0), 1)`
    * `x_c -> BCE(sigma(y_1), x_c)`
    * `y_c -> BCE(sigma(y_2), y_c)`
    * `w -> MSE(y_3, log w)`
    * `h -> MSE(y_4, log h)`

Логарифм для `w` и `h` позволяет одинаково штрафовать одинаковые масштабы ошибки. 

Пусть ошибаемся на 10%: `y_hat = 1.1 * y => log y_hat = log 1.1 + log y`

Ошибка: `log y_hat - log y = log 1.1` вне зависимости от `y`

## 3.2 Сегментация

Необходимо отделить те пиксели, на которых есть объект, от тех пикселей, где объекта нет

Сеть должна выдавать карту скоров, есть ли в данном пикселе или нет фрагмент объекта => выход сети `sigma(y)`

Функций потерь - попиксельная кросс-энтропия: `sum_j BCE(sigma(y_j), t_j)`

## 3.3 Сжатие размерности

Encoder-Decoder архитектура

Функции потерь: `MSE`, попиксельная `BCE` (если интенсивность пикселей от 0 до 1), могут быть и более сложные

## 3.4 SuperResolution

Обучение: сжимаем исходную картинку -> прогон через сеть -> сравнение выхода сети с исходной

# 4. Методы оптимизации

## 4.0 Градиентный спуск
* вычисляем loss на всей выборке
* проблема: необходимо считать значение и градиент loss-функции по всей выборке => много времени на один шаг

## 4.1 Стохастический градиентный спуск
* вычисляем loss по одному примеру 
* проблема: пример может быть неправильно размечен, положительные и отрицательные примеры могут чередоваться, их градиенты будут направлены в разные стороны => неустойчивость

## 4.2 Minibatch градиентный спуск
* вычисляем loss по M (1 << M << размер выборки) объектам
* проблема: вытянутые функции потерь приводят к осцилляциям относительно большой полуоси

## 4.3 Momentum
* `w_(t + 1) = w_t + a * v_t` 
* `v_(t + 1) = b * v_t - grad f(w_t)`
* при градиентных шагах по вытянутой функции потерь проекция скорости на малую полуось будет осциллировать относительно нуля, проекция же на большу полуось будет накапливаться => шаг в направлении большой полуоси будет увеличиваться, и метод сойдется быстрее

## 4.4 Nesterov accelerated gradient
* модификация Momentum: `grad f` вычисляется не в предыдущей точке (`w_t`), а в той, в которую бы мы попали, если бы сделали шаг с той же скоростью (`v_t`)
* таким образом, метод позволяет "не врезаться в стену"
* `w_(t + 1) = w_t + a * v_t` 
* `v_(t + 1) = b * v_t - grad f(w_t + a * v_t)`

## 4.5 Exponential moving average (EMA)
* цель та же, что и у Momentum: найти направление, вдоль которого градиент не осциллирует
* `w_(t + 1) = w_t - a * EMA[(grad f)_t]`
* `EMA[f_t] = (1 - b) * f_t + b * EMA[f_(t - 1)]`

## 4.6 Rprop
* меняем параметры независимо
* `w_(t + 1)_i = w_t_i - a_t_i * sign[(grad f_t)_i ]`
* если произведение i-ой компоненты градиента функции потерь в соседние моменты времени больше нуля, увеличиваем `a_i`, иначе - уменьшаем
* проблема: плохо работает с батчами

## 4.7 RMSprop
* `w_(t + 1) = w_t - a * grad f(w_t) / sqrt[EMA{(grad f) ** 2}]` 
* если по какому-либо параметру функция меняется слишком сильно - стоит спускаться медленнее, если же функция по данному параметру меняется слабо - стоит увеличить шаг

## 4.8 Adam
* метод RMSprop имеет адаптивный шаг для каждого параметра, но напоминает SGD, его можно улучшить с помощью EMA
* `w_(t + 1) = w_t - a * EMA{grad f_t, b1} / sqrt[EMA{(grad f_t) ** 2, b2}]`
* как правило, оптимальные параметры: `a = 3e-4, b1=0.9, b2=0.999`
