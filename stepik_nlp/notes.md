# 1 Введение

## 1.1 Особенности обработки естественных языков

Различия английского и русского языков:

|свойство|английский|русский|
|--------|--------|--------|
|Флективность (словоизменение)|слабая (cat, cats)|сильная (кошка, кошке, кошки)|
|Смысловая омонимия|высокая (well)|высокая (прибрать)|
|Частеречная омонимия|сильная|умеренная (мыла)|
|Порядок слов|фиксированный|свободный|

Стемминг и лемматизация:
* Стемминг - получение основы слова путем отбрасывания окончаний и приставок (часто неплохо работает для английского, для русского есть риск отбросить слишком много от значимого слова)
* Лемматизация - получение нормальной формы. Более сложный, но и более достоверный подход. Используются знания о точной части речи слова => можно с большей уверенностью предположить вид начальной формы

Word sense disambiguation - задача о снятии смысловой неоднозначности<br>
Part Of Speech(POS) tagging - задача о снятии частеречной неоднозначности 

Абстрагироваться от порядка слов в предложении можно посредством синтаксического анализа (построение дерева составляющих или дерева зависимостей)

## 1.2 Группы задач

Три высокоуровневых группы задач:
* лингвистический анализ (разбор структуры текста на разных уровнях)
    * подготовка (графематический анализ) - разбиение на предложения и токены
    * анализ отдельных предложений (POS-tagging, синтаксический и семантический анализ) - разбор от морфологии до семантики
    * анализ целых текстов - разбор связей предложений друг с другом
    * генерация текста
* извлечение признаков (построение векторного представления, графового, сопоставление со словарями)
    * двоичный вектор, описывающий встречаемость слов в документе (чувствителен к опечаткам и случайным словам, теряем информацию о синонимах, слишком большая размерность вектора на небольших выборках может привести к переобучению)
    * вектор вещественных чисел, описывающий встречаемость с учетом частотности (содержит проблемы первого подхода)
    * n-граммы символов и токенов, словосочетания (высокая размерность и разреженность)
    * плотные векторные представления слов, предложений и текстов (word/doc embeddings)
        * матричные разложения и тематическое моделирование (SVD, pLSA, LDA, ARTM)
        * предиктивные дистрибутивно-семантические модели (Word2Vec, FastText)
        * предиктивные модели текста (language model - BERT, ELMo, OpenAI Transformer)
    * ядерные методы и графовые ядра (state-of-the-art качество для некоторых задач, например, извлечение информации из медицинских текстов. посредством ядра можно заложить экспертные знания о предметной области)
* прикладные задачи (классификация, поиск по запросу, поиск похожих, извлечение именованных сущностей)
    * Классификация (для длинных текстов хорошо работает TF-IDF + LogReg, если тексты короткие - нейросети)
    * Поиск
        * TF-IDF + вычисление релевантности BM25
        * Дистрибутивно-семантические модели
        * Лингвистический анализ и алгоритмы для сопоставления структуры текста
        * Когда появляются клики, learning to rank (градиентный бустинг и нейросети)
    * Извлечение структурированной информации (например, из новостей или мед. карт пациентов)
        * системы правил и сопоставление со словарями
        * ядерные методы
        * в меньшей степени нейросети (необходимо от 1000 примеров на каждую сущность)
    * Диалоговые системы - набор различных алгоритмов, выполняющих разные функции
    * Машинный перевод
        * нейросети
        * ранее применялись алгоритмы статистического машинного перевода
    * Эксплоративный анализ (какие тематики есть в коллекции? как они пересекаются и меняются во времени?)
        * тематический анализ (LDA, ARTM)
