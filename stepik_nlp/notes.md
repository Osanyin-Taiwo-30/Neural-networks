# 1. Введение

## 1.1 Особенности обработки естественных языков

Различия английского и русского языков:

|свойство|английский|русский|
|--------|--------|--------|
|Флективность (словоизменение)|слабая (cat, cats)|сильная (кошка, кошке, кошки)|
|Смысловая омонимия|высокая (well)|высокая (прибрать)|
|Частеречная омонимия|сильная|умеренная (мыла)|
|Порядок слов|фиксированный|свободный|

Стемминг и лемматизация:
* Стемминг - получение основы слова путем отбрасывания окончаний и приставок (часто неплохо работает для английского, для русского есть риск отбросить слишком много от значимого слова)
* Лемматизация - получение нормальной формы. Более сложный, но и более достоверный подход. Используются знания о точной части речи слова => можно с большей уверенностью предположить вид начальной формы

Word sense disambiguation - задача о снятии смысловой неоднозначности<br>
Part Of Speech(POS) tagging - задача о снятии частеречной неоднозначности 

Абстрагироваться от порядка слов в предложении можно посредством синтаксического анализа (построение дерева составляющих или дерева зависимостей)

## 1.2 Группы задач

Три высокоуровневых группы задач:
* лингвистический анализ (разбор структуры текста на разных уровнях)
    * подготовка (графематический анализ) - разбиение на предложения и токены
    * анализ отдельных предложений (POS-tagging, синтаксический и семантический анализ) - разбор от морфологии до семантики
    * анализ целых текстов - разбор связей предложений друг с другом
    * генерация текста
* извлечение признаков (построение векторного представления, графового, сопоставление со словарями)
    * двоичный вектор, описывающий встречаемость слов в документе (чувствителен к опечаткам и случайным словам, теряем информацию о синонимах, слишком большая размерность вектора на небольших выборках может привести к переобучению)
    * вектор вещественных чисел, описывающий встречаемость с учетом частотности (содержит проблемы первого подхода)
    * n-граммы символов и токенов, словосочетания (высокая размерность и разреженность)
    * плотные векторные представления слов, предложений и текстов (word/doc embeddings)
        * матричные разложения и тематическое моделирование (SVD, pLSA, LDA, ARTM)
        * предиктивные дистрибутивно-семантические модели (Word2Vec, FastText)
        * предиктивные модели текста (language model - BERT, ELMo, OpenAI Transformer)
    * ядерные методы и графовые ядра (state-of-the-art качество для некоторых задач, например, извлечение информации из медицинских текстов. посредством ядра можно заложить экспертные знания о предметной области)
* прикладные задачи (классификация, поиск по запросу, поиск похожих, извлечение именованных сущностей(NER - named entity recognition))
    * Классификация (для длинных текстов хорошо работает TF-IDF + LogReg, если тексты короткие - нейросети)
    * Поиск
        * TF-IDF + вычисление релевантности BM25
        * Дистрибутивно-семантические модели
        * Лингвистический анализ и алгоритмы для сопоставления структуры текста
        * Когда появляются клики, learning to rank (градиентный бустинг и нейросети)
    * Извлечение структурированной информации (например, из новостей или мед. карт пациентов)
        * системы правил и сопоставление со словарями
        * ядерные методы
        * в меньшей степени нейросети (необходимо от 1000 примеров на каждую сущность)
    * Диалоговые системы - набор различных алгоритмов, выполняющих разные функции
    * Машинный перевод
        * нейросети
        * ранее применялись алгоритмы статистического машинного перевода
    * Эксплоративный анализ (какие тематики есть в коллекции? как они пересекаются и меняются во времени?)
        * тематический анализ (LDA, ARTM)

# 2. Векторная модель текста и классификация длинных текстов

## 2.1 Классическая векторная модель текста

Варианты взвешивания слов:
* по количеству употреблений в документе
    * вес слова зависит от длины документа (в длинных документах слова имеют больший вес, хотя они не более значимы)
    * самые частотные слова - союзы, предлоги, местоимения, которые абсолютно неинформативны
* нормировка вектора документа на его длину (например, по L2-норме)
    * веса слов будут слабее зависить от длины документа, но не перестанут от нее зависеть, поскольку с ростом документа растет и его словарный запас
    * проблема частотных слов остается

Следствия [закона Ципфа](https://ru.wikipedia.org/wiki/%D0%97%D0%B0%D0%BA%D0%BE%D0%BD_%D0%A6%D0%B8%D0%BF%D1%84%D0%B0):
- частотных слов мало, и они неинформативны
- редких слов много, они информативны, но на них сложно опираться

Таким образом, необходимо придерживаться баланса частотности и информативности. За этот баланс отвечает TF-IDF. На практике TF часто логарифмируют. Это позволяет сделать распределение весов слов менее контрастным и уменьшить его дисперсию

Алгоритм взвешивания с TF-IDF:
1. Нормализовать текст (стемминг или лемматизация), выделить базовые элементы (символы, токены, n-граммы)
2. Построить частотный словарь DocCount(word, collection)
3. Проредить словарь по частоте (отбросить слишком редкие и слишком частые токены)
4. Рассчитать TF-IDF

Таким образом, TF-IDF - это способ отбора категориальных признаков

Другой вариант взвешивания признаков - [Pointwise mutual information](https://en.wikipedia.org/wiki/Pointwise_mutual_information) (PMI):

`pmi(l, w) = log[ P(l, w) / (P(l) * P(w)) ] = log[ P(l|w) / P(l) ] = log[ P(w|l) / P(w) ]`

* `P(l)` - уровень априорных ожиданий о появлении события `l`
* `P(l|w)` - уровень ожиданий о появлении события `l` после появления `w`

Применительно к текстовой классификации: `l` - коллекция документов, соответствующая некоторой метке класса `L`; `w` - слово из словаря

Замечания:
* PMI можно применять и к взвешиванию и отбору категориальных признаков и в задачах регрессии после дискретизации целевого распределения, но это уже сложнее
* PMI использует информацию о метках документов, что усложняет его применение в задачах обучения без учителя

## 2.2 классификация с помощью LogReg

модель: `y_hat = sigmoid(w * x + b)`

b (bias, intercept) нужен, чтобы обрабатывать задачи классификации, в которых оптимальная разделяющая поверхность не проходит через начало координат

настройка параметров модели с помощью BCE-loss:

`BCE(y_hat, y)` = `- y * log(y_hat) - (1 - y) * log(1 - y_hat)`

* `- y * log(y_hat)` - штраф в случае ложно-отрицательных(FN) предсказаний (документ принадлежит к положительному классу, а модель выдала низкую вероятность)
* `- (1 - y) * log(1 - y_hat)` - штраф в случае ложно-положительных(FP) предсказаний (документ принадлежит к отрицательному классу, а модель выдала высокую вероятность)

если в качестве предсказания необходима метка класса из `{1, ..., c}`, можно использовать `softmax`:

`y_hat = softmax(W x)`

функция потерь также изменится: классы являются взаимно исключающими, следовательно, достаточно накладывать штраф только для ложно-отрицательных срабатываний:

`CE(y_hat, y) = -sum_i [ y_i * log(y_i_hat) ]`

# 3. Базовые нейросетевые методы работы с текстами

## 3.1 Общий алгоритм работы с текстами с помощью нейросетей

1. разбить текст на токены или символы
2. построить словарь и пронумеровать элементы
3. преобразовать текст в список номеров
4. отображение номеров элементов в векторы, для этого в нейросети хранится таблица эмбеддингов, из которой формируется тензор текста
5. преобразовать тензор текста: `M(length, emb_size) -> M(new_length, new_emb_size)`  с целью получения информации о локальном контексте
6. агрегировать тензор для принятия решений: `Matrix(new_length, new_emb_size) -> Vector(result_size)` с целью получения объекта фиксированной размерности для принятия решений и учета глобального контекста

## 3.2 Дистрибутивная семантика и векторные представления слов

Методы построения матрицы эмбеддингов:
* end-to-end обучение в составе нейросети для решения конечной задачи: нужна большая размеченная выборка
* end-to-end обучение в составе нейросети для моделирования языка: предсказание следующего слова по последовательности слов
* обучение без нейросети в составе дистрибутивно-семантической модели: предсказание типичного контекста употребления слова

Методы дистрибутивной семантики направлены на извлечение смысла слов, анализируя распределение вероятностей совместной встречаемости слов в рамках одного фрагмента текста (или вероятностей встретить одни слова в контексте других)

Дистрибутивная гипотеза:
> You shall know a word by the company it keeps ([Firth, J. R.](https://en.wikipedia.org/wiki/John_Rupert_Firth) 1957:11)

* слово характеризуется своим типичным контекстом
* контекст - соседние слова
* контекст характеризуется матрицей совместной встречаемости слов

||мама|любить|молоток|забивать|
|--------|--------|--------|--------|--------|
|**мама**|-|10^5|10^2|10|
|**любить**|10^5|-|10|10^3|
|**молоток**|10^2|10|-|10^6|
|**забивать**|10|10^3|10^6|-|

принцип работы многих моделей дистрибутивной семантики:
* построить матрицу совместной встречаемости `X`
    * симметричная
    * размера `vocab_size x vocab_size` (`vocab_size ~ 10^5 - 10^6`)
    * разреженность матрицы: 70-95 %
    * значения для частотных слов всегда выше
* сгладить веса в матрице
    * счетчики могут отличаться на несколько порядков
    * частотные слова вносят намного больший вклад
    * примеры сглаживающиx функций: `log(1 + x_ij)`, `PMI`
* построить аппроксимацию: `X ~ W(vocab_size x emb_size) * D(emb_size, vocab_size)`
    * неотрицательное матричное разложение (Non-negative matrix factorization)
    * регрессия: `sum_ij {smooth(x_ij) - w_i.dot(d_j)} ** 2 -> min`
    * классификация (word2vec, fastText): `P(CurWord|CtxWord; W, D) -> max`
    * сингулярное разложение (SVD): `Smooth(X) ~ U * S * V`, `W = U`, `D = S * V`
    * вероятностные модели со скрытыми переменными, латентное размещение Дирихле (LDA)

Модель [GloVe](https://nlp.stanford.edu/projects/glove/): Global Vectors for Word Representation (2014)
* глобальный контекст - матрица совместной встречаемости в документе (все случаи употребления двух слов в документе, а не в рамках предложения или узкого окна)
* линейные модели со среднеквадратичным функционалом ошибки не очень подходят для предсказания переменных, имеющих сильно скошенное распределение с большим разбросом
* чтобы сделать распределение менее контрастным и сжать диапазон значений, счетчики логарифмируются
* регрессия с квадратичным функционалом качества и взвешиванием: `sum_ij f(x_ij) * {ln(1 + x_ij) - w_i.dot(d_j)} ** 2 -> min`
* весовая функция (уменьшает влияние пар с низкой совместной встречаемостью): `f(x) = (x / x_max) ** a if x <= x_max else 1`

Модуль word2vec (2013)
* моделирование условного распределения вероятностей соседних слов
* локальный контекст - окно небольшой ширины
* варианты моделей:
    * Skip Gram: моделирует распределение соседей при условии центрального слова
    * CBOW (continuous bag of words): моделирует распределение центрального слова при соседях
* в модели для каждого слова хранятся и настраиваются 2 вектора:
    * центральный вектор (слово в центре окна)
    * контекстный вектор (слово является контекстом)
* максимизация правдоподобия градиентным спуском
* выходной слой сети - softmax, для оптимизации вычисления softmax применяют:
    * negative sampling
    * hierarchical softmax

## 3.3 Основные виды нейросетевых моделей для обработки текста

Сверточные блоки (Convolutional Neural Networks)
* в отличие от изображений используются одномерные
* выявление паттернов, вне зависимости от их позиции (инвариантность к переносу в пространстве)
* быстрые, хорошо учатся
* могут быть недостаточно гибкими (например, сравнивать первое слово предложения и последнее, игнорируя слова между ними)
* ограничены в ширине контекста (чтобы увеличить длину паттерна, нужно существенно увеличить количество параметров в сети)

Рекуррентные нейросети (Recurrent Neural Networks)
* последовательная обработка текста - слово за словом
* на каждом шаге поддерживается и обновляется вектор скрытого состояния
* могут учитываться длинные зависимости
* длина учитываемых зависимостей не обязательно связана с количеством параметров сети
* медленные, сложно учатся

Блоки объединения (pooling)
* локальный пулинг - уменьшить длину, укрупить детали
* глобальный пулинг - получить вектор, не зависящий от длины исходного текста

Механизм внимания (attention, self-attention)
* учет сколь угодно далеких зависимостей
* попарное сравнение элементов последовательности
* "умная" агрегация
* быстрый, хорошо учатся

Архитектуры с памятью (memory neural network, neural Turing machine)
* учет сложного контекста
* потенциально, решение любых задач
* очень сложно учатся
* емкость памяти невелика

Рекурсивные нейросети (tree-recursive neural networks)
* работают не с последовательностями, а с деревьями
* применяются, например, для того, чтобы сначала выполнить синтаксический анализ, а потом пройтись по дереву и агрегировать информацию из отдельных узлов
* не всегда дают преимущества перед обычными RNN

Графовые сверточные нейросети (graph convolutional neural networks)
* обобщение и сверточных, и рекуррентных сетей на произвольную структуру графа
* вычислительно сложные
