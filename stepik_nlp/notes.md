# 1 Введение

## 1.1 Особенности обработки естественных языков

Различия английского и русского языков:

|свойство|английский|русский|
|--------|--------|--------|
|Флективность (словоизменение)|слабая (cat, cats)|сильная (кошка, кошке, кошки)|
|Смысловая омонимия|высокая (well)|высокая (прибрать)|
|Частеречная омонимия|сильная|умеренная (мыла)|
|Порядок слов|фиксированный|свободный|

Стемминг и лемматизация:
* Стемминг - получение основы слова путем отбрасывания окончаний и приставок (часто неплохо работает для английского, для русского есть риск отбросить слишком много от значимого слова)
* Лемматизация - получение нормальной формы. Более сложный, но и более достоверный подход. Используются знания о точной части речи слова => можно с большей уверенностью предположить вид начальной формы

Word sense disambiguation - задача о снятии смысловой неоднозначности<br>
Part Of Speech(POS) tagging - задача о снятии частеречной неоднозначности 

Абстрагироваться от порядка слов в предложении можно посредством синтаксического анализа (построение дерева составляющих или дерева зависимостей)

## 1.2 Группы задач

Три высокоуровневых группы задач:
* лингвистический анализ (разбор структуры текста на разных уровнях)
    * подготовка (графематический анализ) - разбиение на предложения и токены
    * анализ отдельных предложений (POS-tagging, синтаксический и семантический анализ) - разбор от морфологии до семантики
    * анализ целых текстов - разбор связей предложений друг с другом
    * генерация текста
* извлечение признаков (построение векторного представления, графового, сопоставление со словарями)
    * двоичный вектор, описывающий встречаемость слов в документе (чувствителен к опечаткам и случайным словам, теряем информацию о синонимах, слишком большая размерность вектора на небольших выборках может привести к переобучению)
    * вектор вещественных чисел, описывающий встречаемость с учетом частотности (содержит проблемы первого подхода)
    * n-граммы символов и токенов, словосочетания (высокая размерность и разреженность)
    * плотные векторные представления слов, предложений и текстов (word/doc embeddings)
        * матричные разложения и тематическое моделирование (SVD, pLSA, LDA, ARTM)
        * предиктивные дистрибутивно-семантические модели (Word2Vec, FastText)
        * предиктивные модели текста (language model - BERT, ELMo, OpenAI Transformer)
    * ядерные методы и графовые ядра (state-of-the-art качество для некоторых задач, например, извлечение информации из медицинских текстов. посредством ядра можно заложить экспертные знания о предметной области)
* прикладные задачи (классификация, поиск по запросу, поиск похожих, извлечение именованных сущностей)
    * Классификация (для длинных текстов хорошо работает TF-IDF + LogReg, если тексты короткие - нейросети)
    * Поиск
        * TF-IDF + вычисление релевантности BM25
        * Дистрибутивно-семантические модели
        * Лингвистический анализ и алгоритмы для сопоставления структуры текста
        * Когда появляются клики, learning to rank (градиентный бустинг и нейросети)
    * Извлечение структурированной информации (например, из новостей или мед. карт пациентов)
        * системы правил и сопоставление со словарями
        * ядерные методы
        * в меньшей степени нейросети (необходимо от 1000 примеров на каждую сущность)
    * Диалоговые системы - набор различных алгоритмов, выполняющих разные функции
    * Машинный перевод
        * нейросети
        * ранее применялись алгоритмы статистического машинного перевода
    * Эксплоративный анализ (какие тематики есть в коллекции? как они пересекаются и меняются во времени?)
        * тематический анализ (LDA, ARTM)

# 2. Векторная модель текста и классификация длинных текстов

## 2.1 Классическая векторная модель текста

Варианты взвешивания слов:
* по количеству употреблений в документе
    * вес слова зависит от длины документа (в длинных документах слова имеют больший вес, хотя они не более значимы)
    * самые частотные слова - союзы, предлоги, местоимения, которые абсолютно неинформативны
* нормировка вектора документа на его длину (например, по L2-норме)
    * веса слов будут слабее зависить от длины документа, но не перестанут от нее зависеть, поскольку с ростом документа растет и его словарный запас
    * проблема частотных слов остается

Следствия [закона Ципфа](https://ru.wikipedia.org/wiki/%D0%97%D0%B0%D0%BA%D0%BE%D0%BD_%D0%A6%D0%B8%D0%BF%D1%84%D0%B0):
- частотных слов мало, и они неинформативны
- редких слов много, они информативны, но на них сложно опираться

Таким образом, необходимо придерживаться баланса частотности и информативности. За этот баланс отвечает TF-IDF. На практике TF часто логарифмируют. Это позволяет сделать распределение весов слов менее контрастным и уменьшить его дисперсию

Алгоритм взвешивания с TF-IDF:
1. Нормализовать текст (стемминг или лемматизация), выделить базовые элементы (символы, токены, n-граммы)
2. Построить частотный словарь DocCount(word, collection)
3. Проредить словарь по частоте (отбросить слишком редкие и слишком частые токены)
4. Рассчитать TF-IDF

Таким образом, TF-IDF - это способ отбора категориальных признаков

Другой вариант взвешивания признаков - [Pointwise mutual information](https://en.wikipedia.org/wiki/Pointwise_mutual_information) (PMI):

`pmi(l, w) = log[ P(l, w) / (P(l) * P(w)) ] = log[ P(l|w) / P(l) ] = log[ P(w|l) / P(w) ]`

* `P(l)` - уровень априорных ожиданий о появлении события `l`
* `P(l|w)` - уровень ожиданий о появлении события `l` после появления `w`

Применительно к текстовой классификации: `l` - коллекция документов, соответствующая некоторой метке класса `L`; `w` - слово из словаря

Замечания:
* PMI можно применять и к взвешиванию и отбору категориальных признаков и в задачах регрессии после дискретизации целевого распределения, но это уже сложнее
* PMI использует информацию о метках документов, что усложняет его применение в задачах обучения без учителя

## 2.2 классификация с помощью LogReg

модель: `y_hat = sigmoid(w * x + b)`

b (bias, intercept) нужен, чтобы обрабатывать задачи классификации, в которых оптимальная разделяющая поверхность не проходит через начало координат

настройка параметров модели с помощью BCE-loss:

`BCE(y_hat, y)` = `- y * log(y_hat) - (1 - y) * log(1 - y_hat)`

* `- y * log(y_hat)` - штраф в случае ложно-отрицательных(FN) предсказаний (документ принадлежит к положительному классу, а модель выдала низкую вероятность)
* `- (1 - y) * log(1 - y_hat)` - штраф в случае ложно-положительных(FP) предсказаний (документ принадлежит к отрицательному классу, а модель выдала высокую вероятность)

если в качестве предсказания необходима метка класса из `{1, ..., c}`, можно использовать `softmax`:

`y_hat = softmax(W x)`

функция потерь также изменится: классы являются взаимно исключающими, следовательно, достаточно накладывать штраф только для ложно-отрицательных срабатываний:

`CE(y_hat, y) = -sum_i [ y_i * log(y_i_hat) ]`
